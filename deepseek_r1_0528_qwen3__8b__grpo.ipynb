{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scalixte-mdsol/llm_inferences/blob/main/deepseek_r1_0528_qwen3__8b__grpo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFNfxyrlbLUj"
      },
      "source": [
        "Source: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/DeepSeek_R1_0528_Qwen3_(8B)_GRPO.ipynb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4qEta_8bLUj"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18ZRuDl-bLUk"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    # If you're not in Colab, just use pip install or uv pip install\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    pass # For Colab / Kaggle, we need extra instructions hidden below \\/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "-FKGCZxXaA8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXKch_kpbLUk"
      },
      "outputs": [],
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "!pip install --upgrade -qqq uv\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    # If you're not in Colab, just use pip install!\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    try: import numpy; get_numpy = f\"numpy=={numpy.__version__}\"\n",
        "    except: get_numpy = \"numpy\"\n",
        "    try: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
        "    except: is_t4 = False\n",
        "    get_vllm, get_triton = (\"vllm==0.10.1\", \"triton==3.2.0\") if is_t4 else (\"vllm\", \"triton\")\n",
        "    !uv pip install -qqq --upgrade \\\n",
        "        unsloth {get_vllm} {get_numpy} torchvision bitsandbytes xformers\n",
        "    !uv pip install -qqq {get_triton}\n",
        "!uv pip install transformers==4.55.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIy3QkjW1O4R"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN75nmdx9lvw"
      },
      "source": [
        "Goal: To train `DeepSeek-R1-0528-Qwen3-8B` via GRPO by using OpenR1's Math dataset.\n",
        "\n",
        "Note: DeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base (https://huggingface.co/deepseek-ai/DeepSeek-R1)\n",
        "\n",
        "We also use `langid` for language detection. Our main goal is to force the model to generate reasoning traces in English, and we create a reward function using `langid` to check this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuDt6gPSW_Ds"
      },
      "outputs": [],
      "source": [
        "!pip install langid -qq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unsloth (model load + LoRA)\n",
        "\n",
        "*   Installs langid (for language detection rewards).\n",
        "*   Loads DeepSeek-R1-0528-Qwen3-8B via Unsloth with:\n",
        "     * int4 weights (load_in_4bit=True) to save VRAM,\n",
        "     * fast_inference=True to use vLLM path,\n",
        "     * max_seq_length=1024 context,\n",
        "     * gpu_memory_utilization=0.7 to avoid OOM.\n",
        "\n",
        "\n",
        "* Wraps the base model with LoRA adapters (low-rank trainable layers).\n",
        "* Gradient checkpointing reduces memory during training."
      ],
      "metadata": {
        "id": "2sH9yveVaUR_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkIvEkIIkEyB"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 1024 # Can increase for longer reasoning traces\n",
        "lora_rank = 32 # Larger rank = smarter, but slower\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/DeepSeek-R1-0528-Qwen3-8B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True, # False for LoRA 16bit\n",
        "    fast_inference = True, # Enable vLLM fast inference\n",
        "    max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = 0.7, # Reduce if out of memory\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha = lora_rank*2, # *2 speeds up training\n",
        "    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 1: Try with lora_rank=[8, 16], lower max_seq_length, or set gpu_memory_utilization=0.6"
      ],
      "metadata": {
        "id": "GM7Jj3vbbNvP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IodK13om1O4S"
      },
      "source": [
        "### GRPO Chat Template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cGSRTJo1O4T"
      },
      "source": [
        "Distill Qwen3 from Deepseek has a chat template that is used to format the input and output of the model. This is used to make the model output in a chat format. Including the reasoning step. We have to use that chat template since the model is trained using it.\n",
        "\n",
        "\n",
        "Qwen3’s tokenizer has special tokens like 'think', and role tags.\n",
        "This loop auto-discovers the actual strings (so we don’t hardcode them)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZsp5YKk1O4T"
      },
      "outputs": [],
      "source": [
        "reasoning_start = None\n",
        "reasoning_end = None\n",
        "user_token = None\n",
        "assistant_token = None\n",
        "\n",
        "for token in tokenizer.get_added_vocab().keys():\n",
        "    if \"think\" in token and \"/\" in token:\n",
        "        reasoning_end = token\n",
        "    elif \"think\" in token:\n",
        "        reasoning_start = token\n",
        "    elif \"user\" in token:\n",
        "        user_token = token\n",
        "    elif \"assistant\" in token:\n",
        "        assistant_token = token\n",
        "\n",
        "system_prompt = \\\n",
        "f\"\"\"You are given a problem.\n",
        "Think about the problem and provide your working out.\n",
        "You must think in English.\"\"\"\n",
        "system_prompt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"reasoning_start:\", reasoning_start)\n",
        "print(\"reasoning_end  :\", reasoning_end)\n",
        "print(\"user_token     :\", user_token)\n",
        "print(\"assistant_token:\", assistant_token)\n"
      ],
      "metadata": {
        "id": "UpTSAEERefIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "{\"role\":\"assistant\",\"content\": f\"{reasoning_start}I think it's 2.2{reasoning_end}2\"}\n"
      ],
      "metadata": {
        "id": "GyvgHWw1gpj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = raw = (\n",
        "    f\"{user_token}\\nWhat is 1+1?\\n\"\n",
        "    f\"{assistant_token}\\n{reasoning_start}Let me think...{reasoning_end}2\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "ESMcrXbJhH4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "q7SqK9pOhJru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BciEDYSSYFNj"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.apply_chat_template([\n",
        "    {\"role\" : \"user\", \"content\" : \"What is 1+1?\"},\n",
        "    {\"role\" : \"assistant\", \"content\" : f\"<think>I think it's 2.2</think>2\"},\n",
        "    {\"role\" : \"user\", \"content\" : \"What is 1+1?\"},\n",
        "    {\"role\" : \"assistant\", \"content\" : f\"<think>I think it's 2.2</think>2\"},\n",
        "], tokenize = False, add_generation_prompt = True))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where those variables actually matter in your notebook\n",
        "\n",
        "reasoning_end (&lt;/think&gt;) is used in your regex to extract the final answer (everything after &lt;/think&gt;), and in format rewards.\n",
        "\n",
        "reasoning_start (&lt;think&gt;) + reasoning_end are used in the approximate format reward (counting one open/close).\n",
        "\n",
        "user_token / assistant_token are not used separately as apply_chat_template handles roles."
      ],
      "metadata": {
        "id": "a1g0A_mzh79g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise : How to use &lt;think&gt; with the chat template\n",
        "\n",
        "If you want the generation to start inside a reasoning block, you have to add it yourself:\n",
        "\n",
        "#### Option 1: Append &lt;think&gt; after the serialized template\n",
        "text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
        "text = text + reasoning_start  # e.g., \"&lt;think&gt;\"\n",
        "\n",
        "\"\"\"feed `text` to your generator; model will continue after &lt;/think&gt;\"\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "#### Option 2: Seed the assistant turn with &lt;think&gt; (no generation prompt)\n",
        "seeded = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\",   \"content\": \"Solve (x + 2)^2 = 0\"},\n",
        "    {\"role\": \"assistant\", \"content\": reasoning_start},  # start with \"&lt;think&gt;\"\n",
        "]\n",
        "\n",
        "text = tokenizer.apply_chat_template(seeded, add_generation_prompt=False, tokenize=False)\n",
        "\n",
        "Now the model continues right after &lt;think&gt;.\n",
        "\n"
      ],
      "metadata": {
        "id": "41VN3C2Fi5Ju"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KGgPgk_5S8r"
      },
      "source": [
        "### Data Prep\n",
        "<a name=\"Data\"></a>\n",
        "\n",
        "We're using Hugging Face's [Open R1 Math dataset](https://huggingface.co/datasets/open-r1/DAPO-Math-17k-Processed). You can also utilize OpenAI's famous [GSM8K dataset](https://huggingface.co/datasets/openai/gsm8k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7-eUrQn-OzE"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"open-r1/DAPO-Math-17k-Processed\", \"en\", split = \"train\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "small_dataset_ = dataset.select(range(10))"
      ],
      "metadata": {
        "id": "Rn5_vfiuFOvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b00gUsS-ROW"
      },
      "source": [
        "Let's look at the first row:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siopxjG8-ReF"
      },
      "outputs": [],
      "source": [
        "dataset[0][\"prompt\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGupRQqD-Wcf"
      },
      "outputs": [],
      "source": [
        "dataset[0][\"solution\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmnXj6hn-Ydi"
      },
      "source": [
        "In GSM8K, ee notice all answers like about have a ####, so we extract it. But for the Open R1 dataset, we can skip the below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JJGXKdJ-Zl_"
      },
      "outputs": [],
      "source": [
        "def extract_hash_answer(text):\n",
        "    # if \"####\" not in text: return None\n",
        "    # return text.split(\"####\")[1].strip()\n",
        "    return text\n",
        "extract_hash_answer(dataset[0][\"solution\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K30CygaU-dir"
      },
      "source": [
        "\n",
        "Converts raw rows into the chat format the model expects.\n",
        "Keeps the gold answer in \"answer\" for reward checks.\n",
        "\n",
        "\n",
        "Let's map the dataset! and see the first row:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyEVI972-d3n"
      },
      "outputs": [],
      "source": [
        "small_dataset = small_dataset_.map(lambda x: {\n",
        "    \"prompt\" : [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\",   \"content\": x[\"prompt\"]},\n",
        "    ],\n",
        "    \"answer\": extract_hash_answer(x[\"solution\"]),\n",
        "})\n",
        "# dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "small_dataset[0]"
      ],
      "metadata": {
        "id": "5adhctWlHWTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise: Change your system prompt and apply on the small dataset to see the difference"
      ],
      "metadata": {
        "id": "HkF609iamDto"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9m8eR9T-gMh"
      },
      "source": [
        "We create a regex format to match the reasoning sections and answers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQwjTjNz-gY_"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\"\"\"\n",
        "Build a regex that captures what comes after </think> (the model’s “final answer” area).\n",
        "\"\"\"\n",
        "\n",
        "# Add optional EOS token matching\n",
        "solution_end_regex = rf\"{reasoning_end}(.*)\"\n",
        "\n",
        "match_format = re.compile(solution_end_regex, re.DOTALL)\n",
        "match_format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OycMneOq-iNC"
      },
      "source": [
        "We verify it works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndzHnQ_6-jHt"
      },
      "outputs": [],
      "source": [
        "match_format.findall(\n",
        "    \"Let me think!</think>\"\\\n",
        "    f\"Hence, the solution is 2.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRMDAzDk2x6t"
      },
      "outputs": [],
      "source": [
        "match_format.findall(\n",
        "    \"<think>Let me think!</think>\"\\\n",
        "    f\"\\n\\nHence, the solution is 2\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weOjmO5l-kl3"
      },
      "source": [
        "### Reward functions (format + answer)\n",
        "\n",
        "We now want to create a reward function to match the format exactly - we reward it with 3 points if it succeeds:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgFNXORy-lpO"
      },
      "outputs": [],
      "source": [
        "def match_format_exactly(completions, **kwargs):\n",
        "  # +3.0 if response contains the expected pattern with </think>\n",
        "  # Rewards completions that follow the exact think → final answer structure.\n",
        "    scores = []\n",
        "    for completion in completions:\n",
        "        score = 0\n",
        "        response = completion[0][\"content\"]\n",
        "        # Match if format is seen exactly!\n",
        "        if match_format.search(response) is not None: score += 3.0\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gf69i2WT-m4K"
      },
      "source": [
        "If it fails, we want to reward the model if it at least follows the format partially, by counting each symbol:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUfHzCVx-nGK"
      },
      "outputs": [],
      "source": [
        "def match_format_approximately(completions, **kwargs):\n",
        "  # counts occurrences of <think> and </think>\n",
        "  # Softer reward for partial format adherence.\n",
        "  # +0.5 if exactly one of each, else -1.0 penalties\n",
        "    scores = []\n",
        "    for completion in completions:\n",
        "        score = 0\n",
        "        response = completion[0][\"content\"]\n",
        "        # Count how many keywords are seen - we penalize if too many!\n",
        "        # If we see 1, then plus some points!\n",
        "\n",
        "        # No need to reward <think> since we always prepend it!\n",
        "        score += 0.5 if response.count(reasoning_start) == 1 else -1.0\n",
        "        score += 0.5 if response.count(reasoning_end)   == 1 else -1.0\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wAUWwtE-s6n"
      },
      "source": [
        "We want to extract the generated answer, and reward or penalize it! We also reward it based on how close the answer is to the true one via ratios:\n",
        "\n",
        "Main correctness reward:\n",
        "\n",
        "- exact/close matches → positive,\n",
        "- wrong/missing → penalties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmtI_8gg-uIE"
      },
      "outputs": [],
      "source": [
        "def check_answer(prompts, completions, answer, **kwargs):\n",
        "    # Extract text after </think> and compare against gold \"answer\":\n",
        "    # +5 exact match, +3.5 if strip-equal, +1.5~+2.0 if numeric ratio is close, negative if wrong\n",
        "    question = prompts[0][-1][\"content\"]\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    extracted_responses = [\n",
        "        guess.group(1)\n",
        "        if (guess := match_format.search(r)) is not None else None \\\n",
        "        for r in responses\n",
        "    ]\n",
        "\n",
        "    scores = []\n",
        "    for guess, true_answer in zip(extracted_responses, answer):\n",
        "        score = 0\n",
        "        if guess is None:\n",
        "            scores.append(-2.0)\n",
        "            continue\n",
        "        # Correct answer gets 5 points!\n",
        "        if guess == true_answer:\n",
        "            score += 5.0\n",
        "        # Match if spaces are seen, but less reward\n",
        "        elif guess.strip() == true_answer.strip():\n",
        "            score += 3.5\n",
        "        else:\n",
        "            # We also reward it if the answer is close via ratios!\n",
        "            # Ie if the answer is within some range, reward it!\n",
        "            try:\n",
        "                ratio = float(guess) / float(true_answer)\n",
        "                if   ratio >= 0.9 and ratio <= 1.1: score += 2.0\n",
        "                elif ratio >= 0.8 and ratio <= 1.2: score += 1.5\n",
        "                else: score -= 2.5 # Penalize wrong answers\n",
        "            except:\n",
        "                score -= 4.5 # Penalize\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise: Change the reward scoring mechanism above and run the cells again"
      ],
      "metadata": {
        "id": "tVUzzneRoI4D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atMyfhXh-v3R"
      },
      "source": [
        "Also sometimes it might not be 1 number as the answer, but like a sentence for example \"The solution is $20\" -> we extract 20.\n",
        "\n",
        "We also remove possible commas for example as in 123,456"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVW0kL8q-wL5"
      },
      "outputs": [],
      "source": [
        "# Number extractor (helper)\n",
        "match_numbers = re.compile(\n",
        "    r\".*?[\\s]{0,}([-]?[\\d\\.\\,]{1,})\",\n",
        "    flags = re.MULTILINE | re.DOTALL\n",
        ")\n",
        "print(match_numbers.findall(\"  0.34  \"))\n",
        "print(match_numbers.findall(\"  123,456  \"))\n",
        "print(match_numbers.findall(\"  -0.234  \"))\n",
        "print(match_numbers.findall(\"17\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19KD28CXW_EO"
      },
      "source": [
        "Finally, we will try to enforce the thinking process to be in English. This is a simple version of the `language consistency reward` that is used in DeepSeek R1 paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PU4QSGHW_EO"
      },
      "outputs": [],
      "source": [
        "import langid\n",
        "\n",
        "def get_lang(text: str) -> str:\n",
        "    if not text:\n",
        "        return \"und\"\n",
        "    lang, _ = langid.classify(text)\n",
        "    return lang\n",
        "\n",
        "\n",
        "print(get_lang(\"Hello, How are you\")) # This should return en\n",
        "# print(get_lang(\"Aku berpikir kalau aku adalah kamu\")) # This should return id\n",
        "print(get_lang(\"我在这里\")) # This should return zh"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise: Change to \"zh\" in system prompt and language detector"
      ],
      "metadata": {
        "id": "c9t2Y-Jxoc1C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czn2loIDW_EQ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\"\"\"\n",
        "Encourages English outputs in reasoning/final answer.\n",
        "\"\"\"\n",
        "\n",
        "def format_and_language_reward_func(completions, **kwargs):\n",
        "    scores = []\n",
        "\n",
        "    for completion_item in completions:\n",
        "        if not completion_item or not isinstance(completion_item[0], dict) or \"content\" not in completion_item[0]:\n",
        "            scores.append(-5.0)\n",
        "            print(f\"Warning: Malformed completion item, assigning default low score: {completion_item}\")\n",
        "            continue\n",
        "\n",
        "        content = completion_item[0][\"content\"]\n",
        "\n",
        "        lang = get_lang(content)\n",
        "\n",
        "        if lang == 'en':\n",
        "            score = 5.0\n",
        "        else:\n",
        "            score = -3.0\n",
        "\n",
        "        scores.append(score)\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise:\n",
        "- Flip targets by changing the language code\n",
        "- Change the reward values\n",
        "- Add two language codes"
      ],
      "metadata": {
        "id": "tV2Io20ro6gR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjTfmkTAW_ER"
      },
      "outputs": [],
      "source": [
        "prompts = [\n",
        "    [{\"role\": \"assistant\", \"content\": \"What is the result of (1 + 2) * 4?\"}],\n",
        "    [{\"role\": \"assistant\", \"content\": \"What is the result of (3 + 1) * 2?\"}],\n",
        "]\n",
        "completions = [\n",
        "    [{\"role\": \"assistant\", \"content\": \"<think>The sum of 1 and 2 is 3, which we multiply by 4 to get 12.</think><answer>(1 + 2) * 4 = 12</answer>\"}],\n",
        "    [{\"role\": \"assistant\", \"content\": \"The sum of 3 and 1 is 4, which we multiply by 2 to get 8. So (3 + 1) * 2 = 8.\"}],\n",
        "]\n",
        "format_and_language_reward_func(prompts=prompts, completions=completions)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise: Run all the reward and language code changes mentioned above and print reward values here"
      ],
      "metadata": {
        "id": "omSVNMZdpNRx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbfaaAywNHHh"
      },
      "source": [
        "We now prepare our main function which will print out the generated responses and the true answer, along with another reward function which converts text to float via `float` and sees if it's the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjBFrttr-y1_"
      },
      "outputs": [],
      "source": [
        "global PRINTED_TIMES\n",
        "PRINTED_TIMES = 0\n",
        "global PRINT_EVERY_STEPS\n",
        "PRINT_EVERY_STEPS = 5\n",
        "\n",
        "\"\"\"\n",
        "# Every 5 calls, pretty-print question, gold answer, model response, and extracted number.\n",
        "# Reward +3.5 if numeric match (after stripping commas), else -1.5 or 0\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def check_numbers(prompts, completions, answer, **kwargs):\n",
        "    question = prompts[0][-1][\"content\"]\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    extracted_responses = [\n",
        "        guess.group(1)\n",
        "        if (guess := match_numbers.search(r)) is not None else None \\\n",
        "        for r in responses\n",
        "    ]\n",
        "\n",
        "    scores = []\n",
        "    # Print only every few steps\n",
        "    global PRINTED_TIMES\n",
        "    global PRINT_EVERY_STEPS\n",
        "    if PRINTED_TIMES % PRINT_EVERY_STEPS == 0:\n",
        "        print(\n",
        "            '*'*20 + f\"Question:\\n{question}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\"\n",
        "        )\n",
        "    PRINTED_TIMES += 1\n",
        "\n",
        "    for guess, true_answer in zip(extracted_responses, answer):\n",
        "        if guess is None:\n",
        "            scores.append(-2.5)\n",
        "            continue\n",
        "        # Convert to numbers\n",
        "        try:\n",
        "            true_answer = float(true_answer.strip())\n",
        "            # Remove commas like in 123,456\n",
        "            guess       = float(guess.strip().replace(\",\", \"\"))\n",
        "            scores.append(3.5 if guess == true_answer else -1.5)\n",
        "        except:\n",
        "            scores.append(0)\n",
        "            continue\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgOR3wJ_AyLr"
      },
      "source": [
        "Get the top 90% prompt length so we don't accidentally truncate them!\n",
        "\n",
        "Ie we'll remove the top 10% long prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EgAi4Q5fGE-"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Tokenize and compute lengths L.\n",
        "Keep only the shortest 90% (avoid truncation vs max_seq_length).\n",
        "This preserves prompt integrity and reduces OOM risk.\n",
        "\"\"\"\n",
        "tokenized = small_dataset.map(\n",
        "    lambda x: {\"tokens\" : tokenizer.apply_chat_template(x[\"prompt\"], add_generation_prompt = True, tokenize = True)},\n",
        "    batched = True,\n",
        ")\n",
        "print(tokenizer.decode(tokenized[0][\"tokens\"]))\n",
        "tokenized = tokenized.map(lambda x: {\"L\" : len(x[\"tokens\"])})\n",
        "\n",
        "import numpy as np\n",
        "maximum_length = int(np.quantile(tokenized[\"L\"], 0.9))\n",
        "print(\"Max Length = \", maximum_length)\n",
        "\n",
        "# Filter only samples smaller than 90% max length\n",
        "small_dataset = small_dataset.select(np.where(np.array(tokenized[\"L\"]) <= maximum_length)[0])\n",
        "del tokenized"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise: Change the max_seq_leth and prompt length"
      ],
      "metadata": {
        "id": "BSBA7jznqbAN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-IOMhVg-2AM"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "\n",
        "Now set up GRPO Trainer and all configurations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptqkXK2D4d6p"
      },
      "outputs": [],
      "source": [
        "# Budget the response length so prompt+completion ≤ max_seq_length.#\n",
        "max_prompt_length = maximum_length + 1 # + 1 just in case!\n",
        "max_completion_length = max_seq_length - max_prompt_length\n",
        "\n",
        "from vllm import SamplingParams\n",
        "\n",
        "# Controls how the policy samples multiple completions per prompt (for RL).\n",
        "vllm_sampling_params = SamplingParams(\n",
        "    min_p = 0.1,\n",
        "    top_p = 1.0,\n",
        "    top_k = -1,\n",
        "    seed = 3407,\n",
        "    stop = [tokenizer.eos_token],\n",
        "    include_stop_str_in_output = True,\n",
        ")\n",
        "\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "training_args = GRPOConfig(\n",
        "    vllm_sampling_params = vllm_sampling_params,\n",
        "    temperature = 1.0,\n",
        "    learning_rate = 5e-6,\n",
        "    weight_decay = 0.01,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type = \"linear\",\n",
        "    optim = \"adamw_8bit\",\n",
        "    logging_steps = 1,\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
        "    num_generations = 2, # Decrease if out of memory, samples per prompt per step\n",
        "    max_prompt_length = max_prompt_length,\n",
        "    max_completion_length = max_completion_length,\n",
        "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
        "    max_steps = 4, # ← tiny demo run (use more for real training)\n",
        "    save_steps = 1,\n",
        "    report_to = \"none\", # Can use Weights & Biases\n",
        "    output_dir = \"outputs\",\n",
        "\n",
        "    # For optional training + evaluation\n",
        "    # fp16_full_eval = True,\n",
        "    # per_device_eval_batch_size = 4,\n",
        "    # eval_accumulation_steps = 1,\n",
        "    # eval_strategy = \"steps\",\n",
        "    # eval_steps = 1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise:\n",
        "- Change top_p, temperature values"
      ],
      "metadata": {
        "id": "3EPiCWNBrAmn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GRPO needs ≥2 samples per prompt to work properly.**\n",
        "So we keep `per_device_train_batch_size=1` (to save VRAM) but set **`num_generations=2`** so each prompt is sampled twice and the algorithm can do its **group-relative** math.\n",
        "\n",
        "* ** `per_device_train_batch_size` = how many **prompts** you feed the policy at once.\n",
        "\n",
        "* `num_generations` = how many **completions per prompt** you sample each step (k). GRPO then compares those k completions for the *same prompt* to compute **relative advantages**:\n",
        "\n",
        "  $$\n",
        "  \\text{adv}_i = r_i - \\text{mean}(r_{1..k}) \\quad (\\text{often normalized by std})\n",
        "  $$\n",
        "\n",
        "  If **k=1**, there’s no “group” → the relative term collapses; training degenerates toward REINFORCE/KL and the signal is much weaker.\n",
        "\n",
        "* **Why choose k=2 (the minimum that works)**\n",
        "\n",
        "  * Satisfies the **group** requirement (you can rank/center rewards).\n",
        "  * Much cheaper in VRAM than bumping batch size (since the **prompt is reused** across the two samples).\n",
        "  * Plays nicely with vLLM’s sampler (sampling multiple continuations of the same prompt is efficient).\n",
        "\n",
        "* **Effective group size & memory**:\n",
        "\n",
        "  Effective samples per step ≈ `batch_size * num_generations`.\n",
        "  With `batch_size=1, num_generations=2`, you get a group of 2 per prompt—**lowest VRAM** option that still gives GRPO a valid learning signal.\n",
        "\n",
        "**Summary:** Keep batch size low to fit memory; set `num_generations≥2` so GRPO can compare multiple completions per prompt and produce meaningful gradients.\n"
      ],
      "metadata": {
        "id": "EsDNZNgrsAnw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise: Change per_device_train_batch_size = 2. What happens?"
      ],
      "metadata": {
        "id": "W7x3iXSNslAF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9Mv8UZO5hz-"
      },
      "source": [
        "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
        "\n",
        "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
        "\n",
        "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
        "|------|---------------|-----------|------------|-------------------|----------|\n",
        "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
        "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
        "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzOuSVCL_GA9"
      },
      "outputs": [],
      "source": [
        "# For optional training + evaluation\n",
        "# new_dataset = dataset.train_test_split(test_size = 0.01)\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [\n",
        "        match_format_exactly,\n",
        "        match_format_approximately,\n",
        "        check_answer,\n",
        "        check_numbers,\n",
        "        format_and_language_reward_func,\n",
        "    ],\n",
        "    args = training_args,\n",
        "    train_dataset = small_dataset,\n",
        "\n",
        "    # For optional training + evaluation\n",
        "    # train_dataset = new_dataset[\"train\"],\n",
        "    # eval_dataset = new_dataset[\"test\"],\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise: Increase num_generations, what happens ?"
      ],
      "metadata": {
        "id": "cR-5EVWNuCQ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gradient Acculumation\n",
        "\n",
        "`gradient_accumulation_steps` tells the trainer to **split one optimizer update across multiple forward/backward passes**. You do several small mini-batches, **accumulate** their gradients, and only then call `optimizer.step()` once.\n",
        "\n",
        "If gradient_accumulation_steps = 1, there’s no accumulation—you do one forward + backward, then immediately optimizer.step() and zero_grad().\n",
        "\n",
        "#### Why it exists:\n",
        "\n",
        "* **Fits memory:** You can keep `per_device_train_batch_size` small (e.g., 1) to avoid OOM, but still get a **larger effective batch** by accumulating over several steps.\n",
        "* **Smoother training:** Larger effective batch often stabilizes rewards/loss.\n",
        "\n",
        "#### How it works (conceptually)\n",
        "\n",
        "If `gradient_accumulation_steps = G`:\n",
        "\n",
        "1. For `i = 1..G`:\n",
        "\n",
        "   * Run forward → compute loss\n",
        "   * `loss.backward()` (gradients **add up** in model params)\n",
        "2. After G mini-batches:\n",
        "\n",
        "   * `optimizer.step()`\n",
        "   * `optimizer.zero_grad()`\n",
        "\n",
        "So you do **G backward passes per update**.\n",
        "\n",
        "#### Effective batch size\n",
        "\n",
        "For GRPO (which samples multiple completions per prompt):\n",
        "\n",
        "```\n",
        "effective_prompts_per_update   = per_device_train_batch_size × gradient_accumulation_steps\n",
        "effective_completions_per_update = effective_prompts_per_update × num_generations\n",
        "```\n",
        "\n",
        "With your defaults:\n",
        "\n",
        "* `per_device_train_batch_size = 1`\n",
        "* `gradient_accumulation_steps = 1`\n",
        "* `num_generations = 2`\n",
        "\n",
        "→ `effective_prompts_per_update = 1 × 1 = 1`\n",
        "→ `effective_completions_per_update = 1 × 2 = 2`\n",
        "\n",
        "If you set `gradient_accumulation_steps = 4` (keep batch size = 1):\n",
        "→ `effective_prompts_per_update = 1 × 4 = 4`\n",
        "→ `effective_completions_per_update = 4 × 2 = 8`\n",
        "…without raising peak memory like a real batch of 4 would.\n",
        "\n",
        "#### Memory vs speed trade-off\n",
        "\n",
        "* **Higher `gradient_accumulation_steps`**:\n",
        "\n",
        "  * Lower **peak** VRAM than increasing `per_device_train_batch_size`\n",
        "  * Larger effective batch (better signal)\n",
        "  * Slower wall-clock per optimizer update (you do more forwards/backwards before stepping)\n",
        "* **Increasing `per_device_train_batch_size`** instead raises peak memory more (multiple prompts’ KV caches live at once)."
      ],
      "metadata": {
        "id": "gDO7TtBGuXCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise: Set gradient_accumulation_steps to [2, 3, 4], observe the differences"
      ],
      "metadata": {
        "id": "7voKmN9jvFkZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlaUdxC_VHpz"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference (baseline vs LoRA)\n",
        "Now let's try the model we just trained! First, let's first try the model without any GRPO trained:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtcz_lpbVC92"
      },
      "outputs": [],
      "source": [
        "text = \"What is the sqrt of 101?\"\n",
        "\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 1.0,\n",
        "    top_k = 50,\n",
        "    max_tokens = 1024,\n",
        ")\n",
        "# Baseline generation (no LoRA) with vLLM-style fast_generate.\n",
        "output = model.fast_generate(\n",
        "    [text],\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = None,\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Colxz9TAVMsi"
      },
      "source": [
        "And now with the LoRA we just trained with GRPO - we first save the LoRA first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AL-BcuB1VLIv"
      },
      "outputs": [],
      "source": [
        "model.save_lora(\"grpo_lora\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4LMOBl8boGX"
      },
      "source": [
        "Verify LoRA is actually trained!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SfdI-ERbpiw"
      },
      "outputs": [],
      "source": [
        "# # Verify LoRA has non-zero weights\n",
        "from safetensors import safe_open\n",
        "\n",
        "tensors = {}\n",
        "with safe_open(\"grpo_lora/adapter_model.safetensors\", framework = \"pt\") as f:\n",
        "    # Verify both A and B are non zero\n",
        "    for key in f.keys():\n",
        "        tensor = f.get_tensor(key)\n",
        "        n_zeros = (tensor == 0).sum() / tensor.numel()\n",
        "        assert(n_zeros.item() != tensor.numel())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwpbwnDBVRLg"
      },
      "source": [
        "Now we load the LoRA and test. We tested without using our custom system prompt which should not (or minimal) affect toward the model's original reasoning ability.:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6lXk47v1O4b"
      },
      "outputs": [],
      "source": [
        "# # Inference WITH LoRA (no system prompt)\n",
        "messages = [\n",
        "    {\"role\": \"user\",   \"content\": \"Solve (x + 2)^2 = 0\"},\n",
        "]\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    tokenize = False,\n",
        ")\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 1.0,\n",
        "    top_k = 50,\n",
        "    max_tokens = 2048,\n",
        ")\n",
        "output = model.fast_generate(\n",
        "    text,\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = model.load_lora(\"grpo_lora\"),\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g399AC2B1O4b"
      },
      "source": [
        "Next, let's test using our system prompt which should use the new language :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zf_OY5WMVOxF"
      },
      "outputs": [],
      "source": [
        "# Inference WITH system prompt (encourages English chain-of-thought)\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\",   \"content\": \"Solve (x + 2)^2 = 0\"},\n",
        "]\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    tokenize = False,\n",
        ")\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 1.0,\n",
        "    top_k = 50,\n",
        "    max_tokens = 2048,\n",
        ")\n",
        "output = model.fast_generate(\n",
        "    text,\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = model.load_lora(\"grpo_lora\"),\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad5qCZMsW_Ed"
      },
      "source": [
        "Lets compare our results with system prompt but without our LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee10WWhDW_Ee"
      },
      "outputs": [],
      "source": [
        "# Control comparison (same prompt, no LoRA)\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\",   \"content\": \"Solve (x + 2)^2 = 0\"},\n",
        "]\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    tokenize = False,\n",
        ")\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 1.0,\n",
        "    top_k = 50,\n",
        "    max_tokens = 2048,\n",
        ")\n",
        "output = model.fast_generate(\n",
        "    text,\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = None,\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mini language comparison (4 samples)"
      ],
      "metadata": {
        "id": "Qba4KLc9wrs2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYqpfCF0W_Ee"
      },
      "source": [
        "Let's take 4 samples, and compare the the amount of using our LoRA and not using it, and see which one has better amount of correct language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJmztPHdW_Ef"
      },
      "outputs": [],
      "source": [
        "sample_dataset = small_dataset.shuffle(seed = 3407).select(range(4))\n",
        "sample_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise: Increase the dataset to 10, and sample 6 data-points from it"
      ],
      "metadata": {
        "id": "T6P6QhALwwar"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jD4c_LWW_Ef"
      },
      "outputs": [],
      "source": [
        "with_lora_id_count = 0\n",
        "without_lora_id_count = 0\n",
        "\n",
        "print(\"Comparing language usage with and without LoRA on 4 samples:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, sample in enumerate(sample_dataset):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": sample[\"prompt\"][1][\"content\"]},\n",
        "    ]\n",
        "\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=False,\n",
        "    )\n",
        "\n",
        "    output_with_lora = model.fast_generate(\n",
        "        text,\n",
        "        sampling_params=sampling_params,\n",
        "        lora_request=model.load_lora(\"grpo_lora\"),\n",
        "    )[0].outputs[0].text\n",
        "\n",
        "    output_without_lora = model.fast_generate(\n",
        "        text,\n",
        "        sampling_params=sampling_params,\n",
        "        lora_request=None,\n",
        "    )[0].outputs[0].text\n",
        "\n",
        "    lang_with_lora = get_lang(output_with_lora)\n",
        "    lang_without_lora = get_lang(output_without_lora)\n",
        "\n",
        "    if lang_with_lora == 'id':\n",
        "        with_lora_id_count += 1\n",
        "    if lang_without_lora == 'id':\n",
        "        without_lora_id_count += 1\n",
        "\n",
        "    # Print progress every 4 samples\n",
        "    if (i + 1) % 1 == 0:\n",
        "        print(f\"Processed {i + 1}/4 samples...\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"RESULTS:\")\n",
        "print(f\"With LoRA - English responses: {with_lora_id_count}/4 ({with_lora_id_count/20*100:.1f}%)\")\n",
        "print(f\"Without LoRA - English responses: {without_lora_id_count}/4 ({without_lora_id_count/20*100:.1f}%)\")\n",
        "print(f\"Improvement: +{with_lora_id_count - without_lora_id_count} English responses with LoRA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise: Can you all spot the bug in the above cell?"
      ],
      "metadata": {
        "id": "nf6ArJdQxCGg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NUEmHFSYNTp"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving to float16 for VLLM\n",
        "\n",
        "Select `merged_16bit` for float16 or `merged_4bit` for int4. Can also use `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjXGTkp7YNtB"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False:\n",
        "    model.save_pretrained(\"model\")\n",
        "    tokenizer.save_pretrained(\"model\")\n",
        "if False:\n",
        "    model.push_to_hub(\"hf/model\", token = \"\")\n",
        "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52WMb3k_YPt8"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "From Unsloth:\n",
        "\n",
        "- Export to GGUF (for llama.cpp, LM Studio, Ollama).\n",
        "- Offers several quantization flavors (q8_0, q4_k_m, q5_k_m, etc.).\n",
        "\n",
        "\n",
        "To save to `GGUF` / `llama.cpp`, library supports it natively now: clone `llama.cpp` and default save it to `q8_0`. It allows all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on the [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyEjW-WuYQIm"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxaYP7QBW_Ej"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp.\n",
        "\n",
        "And we're done! If you have any questions on Unsloth, they have a [Discord](https://discord.gg/unsloth) channel!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Expert Parallelism\n",
        "\n",
        "DeepSeek-V3/R1 models replace MLP Layers with MoE Layers. An MoE Layer has 256 routed experts and one shared expert. Each token is dispatched to 8 different routed experts for computation, and the results are weighted summed. Each token also computes in the shared expert, and the result is added to the result from the routed experts.\n",
        "\n",
        "Expert Parallelism (EP) serves as the typical sharding approach for MoE Layers, with each GPU managing 256 / EP routed experts while maintaining a copy of the shared expert. Compared to TP, the advantage of EP is that it can distribute computation across more GPUs, reducing the computation and memory usage per GPU.\n",
        "\n",
        "Before performing expert computation, all GPUs need to perform an AllToAll communication to dispatch tokens to the GPUs where the corresponding experts are located; after expert computation, another AllToAll communication is needed to collect computation results from various GPUs and perform weighted summation."
      ],
      "metadata": {
        "id": "CSC7Xlhm1HO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Few Inference-time optimizations used in DeepSeek-V3**\n",
        "\n",
        "1. **Multi-Head Latent Attention (MLA)** — *smaller KV cache, faster decode*\n",
        "   V3 keeps the MLA attention from V2 specifically **for efficient inference**. MLA compresses Q/K/V into a latent space so the **KV cache you store during generation is much smaller** (low rance KV cache), which cuts memory traffic and speeds token-by-token decoding. (V2 quantified this: **\\~93.3% KV-cache reduction** and up to **\\~5.76× max throughput**; V3 inherits MLA for inference.) ([arXiv][1], [GitHub][2])\n",
        "\n",
        "2. **Sparse MoE at runtime** — *compute only a few experts/token*\n",
        "   V3 is a 671B-param MoE with only **\\~37B active per token**. Because only a small subset of experts runs for each token, you do less matmul/communication per decode step than a dense model of similar total size. This lowers per-token FLOPs and improves throughput at inference. ([arXiv][1])\n",
        "\n",
        "3. **Multi-Token Prediction (MTP) + speculative decoding** — *lower latency decode*\n",
        "   V3 trains auxiliary MTP modules so the model can **predict multiple future tokens**. At inference you can repurpose them for **speculative decoding** (propose extra tokens and verify). The paper reports **85–90% acceptance** for the second token and about **\\~1.8× tokens/s** when used this way. (If you don’t want speculation, you can drop the MTP modules and run normally.) ([arXiv][1])"
      ],
      "metadata": {
        "id": "Qg4vrhgU33a2"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}