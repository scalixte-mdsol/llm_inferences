{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scalixte-mdsol/llm_inferences/blob/main/flash_paged_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGoOSwqee8Y0"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Minimal attention showdown\n",
        "# =========================\n",
        "import math, time, torch, torch.nn.functional as F\n",
        "\n",
        "# assert torch.cuda.is_available(), \"Enable GPU in Colab: Runtime > Change runtime type > GPU\"\n",
        "# device = \"cuda\"\n",
        "# dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device == \"cuda\":\n",
        "    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "else:\n",
        "    dtype = torch.float32  # CPU prefers fp32 for correctness\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flash Attention"
      ],
      "metadata": {
        "id": "o7MqAFw8-SAw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What the function is for (big picture)**\n",
        "\n",
        "It computes causal self-attention for a full sequence (“prefill”) without ever materializing the huge S × S attention matrix.\n",
        "Instead, it processes the sequence in tiles (blocks of queries and keys/values) and uses an online softmax trick to stitch results together numerically stably. This is the core idea behind FlashAttention—just written in simple PyTorch, not a fused CUDA kernel."
      ],
      "metadata": {
        "id": "HdXMjt-EjDNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inputs & outputs**\n",
        "\n",
        "q, k, v: tensors with shape (B, H, S, D)\n",
        "\n",
        "B = batch size, H = number of heads, S = sequence length, D = head dimension.\n",
        "\n",
        "causal=True: enforce “no peeking into the future.”\n",
        "\n",
        "q_block, k_block: tile sizes for queries and keys/values.\n",
        "\n",
        "Returns: tensor (B, H, S, D) — the attended values for every token."
      ],
      "metadata": {
        "id": "dbbBKcMGjSAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bKXd_5eYjgcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 1) Simple \"flash-like\" attention (prefill): tiling + online softmax ----------\n",
        "@torch.no_grad()\n",
        "def flash_attn_simple(q, k, v, *, causal=True, q_block=128, k_block=128):\n",
        "    \"\"\"\n",
        "    Educational flash-like attention (no custom kernels).\n",
        "    q,k,v: (B,H,S,D); returns (B,H,S,D)\n",
        "    \"\"\"\n",
        "    B, H, S, D = q.shape\n",
        "\n",
        "    # Prepare output & scaling. The scale is standard in attention: softmax(QKᵀ / √D) V.\n",
        "    scale = 1.0 / math.sqrt(D)\n",
        "\n",
        "    out = torch.zeros(B, H, S, D, device=q.device, dtype=q.dtype)\n",
        "\n",
        "    # Loop over queries in tiles. We’ll compute attention for a small chunk of queries Qb at a time to save memory.\n",
        "    for qs in range(0, S, q_block):\n",
        "        qe = min(qs + q_block, S)\n",
        "        q_blk = q[:, :, qs:qe, :]  # shape (B,H,Qb,D)\n",
        "\n",
        "        # Running max/log-sum-exp accumulators for online softmax\n",
        "        \"\"\"\n",
        "        What are qs, qe, ks, ke?\n",
        "\n",
        "        You’re processing the sequence in tiles (blocks).\n",
        "        qs:qe is the query tile’s global index range.\n",
        "        ks:ke is the key/value tile’s global index range.\n",
        "        So:\n",
        "        Qb = qe - qs queries in this tile,\n",
        "        Kb = ke - ks keys in this tile.\n",
        "\n",
        "        Initialize running statistics for online softmax\n",
        "        These three hold the softmax state as we accumulate contributions from many key/value tiles:\n",
        "        m: the current max of attention scores seen so far (per query position).\n",
        "        l: the denominator of softmax (sum of exponentials), rescaled by m.\n",
        "        o: the weighted sum of values (numerator), also rescaled by m.\n",
        "        \"\"\"\n",
        "        m = torch.full((B, H, qe - qs, 1), -float(\"inf\"), device=q.device, dtype=q.dtype)\n",
        "        l = torch.zeros(B, H, qe - qs, 1, device=q.device, dtype=q.dtype)\n",
        "        o = torch.zeros(B, H, qe - qs, D, device=q.device, dtype=q.dtype)\n",
        "\n",
        "\n",
        "        # Loop over keys/values in tiles. This computes a Qb × Kb slice of the big QKᵀ matrix.\n",
        "        for ks in range(0, S, k_block):\n",
        "            ke = min(ks + k_block, S)\n",
        "            k_blk = k[:, :, ks:ke, :]\n",
        "            v_blk = v[:, :, ks:ke, :]\n",
        "            scores = torch.einsum(\"bhqd,bhkd->bhqk\", q_blk, k_blk) * scale  # (B,H,Qb,Kb)\n",
        "\n",
        "            # Apply causal masking (if requested). Any key to the right of a query (future) is masked out. This keeps generation valid.\n",
        "            if causal:\n",
        "                \"\"\"\n",
        "                For each query position (row) and key position (column), it checks: is this key in the future?\n",
        "                If k_idx > q_idx, that key is to the right of (i.e., after) the query → mask it out.\n",
        "                Because we use > (strictly greater), a token can attend to itself (k == q is allowed). Using >= would forbid self-attention, which we don’t want.\n",
        "\n",
        "                \"\"\"\n",
        "                # mask future keys inside the current tiles\n",
        "                \"\"\"\n",
        "                torch.arange(qs, qe) is [qs, qs+1, ..., qe-1] — the global positions for the queries in this tile.\n",
        "                torch.arange(ks, ke) is [ks, ks+1, ..., ke-1] — the global positions for the keys in this tile.\n",
        "                unsqueeze(-1) makes q_idx a column of shape (Qb, 1).\n",
        "                unsqueeze(0) makes k_idx a row of shape (1, Kb).\n",
        "                \"\"\"\n",
        "                q_idx = torch.arange(qs, qe, device=q.device).unsqueeze(-1)  # (Qb,1) # global positions for current queries\n",
        "                k_idx = torch.arange(ks, ke, device=q.device).unsqueeze(0)  # (1,Kb)  # global positions for current keys\n",
        "                # When you compare them, PyTorch uses broadcasting to form a full (Qb, Kb) matrix of comparisons.\n",
        "                mask = (k_idx > q_idx)  # (Qb,Kb)  # # future keys relative to each query\n",
        "                # The code lifts this to match the attention score tensor (B, H, Qb, Kb):\n",
        "                scores = scores.masked_fill(mask.unsqueeze(0).unsqueeze(0), float(\"-inf\"))\n",
        "\n",
        "            \"\"\"\n",
        "            Online softmax merge (numerical stability)\n",
        "            This is the heart of the method:\n",
        "            If you did softmax across all keys at once, you’d use\n",
        "            softmax(scores) = exp(scores - max) / sum(exp(scores - max)).\n",
        "            Here we only see a slice of keys at a time, so we:\n",
        "            keep a running max m and running sums (o, l),\n",
        "            rescale the old accumulators to the new max (m_new) before adding the new chunk.\n",
        "            After processing all K/V tiles, (o / l) equals the same result you’d get from the full softmax.\n",
        "            \"\"\"\n",
        "\n",
        "            m_new = torch.maximum(m, scores.amax(dim=-1, keepdim=True))\n",
        "            exp_scores = torch.exp(scores - m_new)\n",
        "            o = o * torch.exp(m - m_new) + torch.einsum(\"bhqk,bhkd->bhqd\", exp_scores, v_blk)\n",
        "            l = l * torch.exp(m - m_new) + exp_scores.sum(dim=-1, keepdim=True)\n",
        "            m = m_new\n",
        "\n",
        "        # Write the finished block to output. That’s the final softmax-weighted sum of V for those query positions.\n",
        "        out[:, :, qs:qe, :] = o / l\n",
        "    return out"
      ],
      "metadata": {
        "id": "cww-JA_ue9VE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why this works (and why it’s useful)**\n",
        "\n",
        "Memory savings: We never build the full S × S attention matrix. We only hold small Qb × Kb pieces, so peak memory is much lower.\n",
        "\n",
        "Numerical stability: The running-max (log-sum-exp) trick lets us merge tiles without losing precision or blowing up exponentials.\n",
        "\n",
        "Causal correctness: Masking is done per tile using global positions, so the lower-triangular constraint is preserved across tiles."
      ],
      "metadata": {
        "id": "BnpUrmgAlj_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tuning knobs (and trade-offs)**\n",
        "\n",
        "q_block, k_block:\n",
        "\n",
        "Smaller blocks → lower peak memory, but more loops → slower (especially on GPU, due to Python overhead and many kernel launches).\n",
        "\n",
        "Larger blocks → faster, but need more memory.\n",
        "\n",
        "@torch.no_grad():\n",
        "\n",
        "This function is wrapped with no_grad (good for demos/inference/benchmarks).\n",
        "\n",
        "If you want to train with it, remove @torch.no_grad() so gradients are tracked (but expect it to be much slower than fused kernels)."
      ],
      "metadata": {
        "id": "XXHOyWZjlnvo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paged Attention"
      ],
      "metadata": {
        "id": "n2BZTUrY-V6T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paged attention is an inference-time way to run attention for decoder-only LLMs that’s optimized for long contexts and many concurrent requests. It combines two ideas:\n",
        "\n",
        "Paged KV cache (memory layout):\n",
        "Store each sequence’s keys/values (K/V) in fixed-size pages from a global pool. Each sequence has a page table (a list of page IDs). When a sequence grows, you append into its current tail page; if it fills, you allocate a new page. When the sequence finishes, you return its pages to the pool.\n",
        "Why: avoids big memcopies and fragmentation, and enables “continuous batching” of requests with different lengths.\n",
        "\n",
        "Paged decode (compute):\n",
        "For each new token (so one query per head), the kernel walks the page table and computes attention page by page instead of over the whole context at once. It uses an online (streaming) softmax to merge each page’s contribution stably, so you never need to materialize the full logits vector.\n",
        "Why: keeps peak activations low and scales well as context grows."
      ],
      "metadata": {
        "id": "_fyt0Yk9Bpvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### How a decode step works (conceptually)\n",
        "\n",
        "For a single new query $q$:\n",
        "\n",
        "1. Look up the sequence’s **page IDs**: `[p0, p1, …, pN]`.\n",
        "2. For each page `pi`:\n",
        "\n",
        "   * Load that page’s `K_i, V_i`.\n",
        "   * Compute logits $q K_i^\\top / \\sqrt{d}$.\n",
        "   * Update running softmax **max/denominator/numerator** (log-sum-exp trick) with this page’s chunk.\n",
        "3. After all pages, output $\\text{softmax}(qK^\\top)V$ from the accumulated numerator/denominator.\n",
        "4. Append the new token’s K/V to the sequence’s **tail page** (allocate a new page if the tail is full).\n",
        "\n",
        "> The **math result** is the same as computing attention over the whole context; it’s just streamed in pages for memory and scheduling efficiency.\n",
        "\n",
        "\n",
        "\n",
        "### Why it’s useful\n",
        "\n",
        "* **Continuous batching:** Mix sequences of very different lengths without copying K/V around.\n",
        "* **Long context friendly:** O(S) memory for activations during decode (only one page in flight).\n",
        "* **High throughput serving:** Less memory churn, fewer large reallocations.\n",
        "\n",
        "\n",
        "\n",
        "### How it differs from other terms\n",
        "\n",
        "* **Paged attention vs. Flash attention:**\n",
        "  *Flash* is a fused kernel that speeds up full attention inside a block (great for **prefill**). *Paged attention* is about how you **store** K/V and **stream** compute during **decode**. Systems often use both: Flash for prefill; Paged for decode.\n",
        "* **Paged decode vs. Paged KV cache:**\n",
        "  *Paged decode* = the **compute** strategy (process K/V in chunks with online softmax).\n",
        "  *Paged KV cache* = the **memory** strategy (fixed-size pages + allocator + page tables).\n",
        "  In production, kernels read **directly from pages** while doing the paged decode.\n",
        "\n",
        "\n",
        "### Tiny pseudocode (decode for one new token)\n",
        "\n",
        "```python\n",
        "# page_table: list of page IDs for this sequence\n",
        "m = -inf         # running max logit\n",
        "num = 0          # running numerator\n",
        "den = 0          # running denominator\n",
        "for page_id in page_table:\n",
        "    K_page, V_page = load_page(page_id)           # (H, PAGE, D)\n",
        "    logits = (q @ K_page.T) / sqrt(D)             # (H, PAGE)\n",
        "    cur_max = max_over_page(logits)               # (H, 1)\n",
        "    new_max = max(m, cur_max)\n",
        "    num = num * exp(m - new_max) + exp(logits - new_max) @ V_page\n",
        "    den = den * exp(m - new_max) + sum(exp(logits - new_max), axis=page)\n",
        "    m = new_max\n",
        "out = num / den                                   # (H, D)\n",
        "```\n",
        "\n",
        "### Practical notes\n",
        "\n",
        "* **Page sizes:** The **storage** page size (for KV cache) and the **compute** chunk size don’t have to be equal, but implementations pick sizes that map well to GPU tiles.\n",
        "* **Libraries:** vLLM/FlashInfer implement paged attention with **fused CUDA kernels** and a KV allocator; PyTorch SDPA can be used for prefill (Flash backend) and for simple decode, but the full paging system is what unlocks serving efficiency at scale.\n",
        "\n",
        "**Summary:** *Paged attention streams attention over a KV cache organized in fixed-size pages, letting servers handle very long, many, and variable-length sequences efficiently during decode.*\n"
      ],
      "metadata": {
        "id": "BjfWNbXtB5yh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* **Flash Attention** = a **turbo-charged kernel** to compute attention fast and memory-efficiently when you already have contiguous Q/K/V for a block. It’s about **how the math is computed** (fused, tiled on-chip, online softmax).\n",
        "* **Paged Attention** = a **serving strategy** for the **decode step** that organizes and reads the **KV cache** in fixed-size **pages** so many sequences of different lengths can be handled **without copying**. It’s about **how K/V are stored and streamed** during inference.\n",
        "\n",
        "## Side-by-side (at a glance)\n",
        "\n",
        "| Dimension               | **Flash Attention**                                                                              | **Paged Attention**                                                                                                          |\n",
        "| ----------------------- | ------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------- |\n",
        "| Main goal               | Make the attention **computation** itself fast & low-memory (fused kernel).                      | Make **inference with long/variable contexts** efficient via a paged **KV cache** and streaming compute.                     |\n",
        "| Used when               | **Prefill** (full S×S) and sometimes training; also works for decode if you have contiguous K/V. | **Decode** (1 new token) across long history and many concurrent requests.                                                   |\n",
        "| What it optimizes       | **Kernel efficiency**: fuses QKᵀ → softmax → (·V), tiles in SRAM, online softmax.                | **Memory layout & scheduling**: split K/V into **pages**, keep a **page table** per sequence, reuse pages, avoid K/V copies. |\n",
        "| Input layout assumption | Typically **contiguous** Q/K/V for the current block.                                            | K/V can be **non-contiguous**, scattered across pages; kernel **walks the page table**.                                      |\n",
        "| Memory during compute   | Very low **activation** memory for full attention (doesn’t materialize S×S).                     | Low **working** memory at decode (process one page at a time); total K/V stored in pages.                                    |\n",
        "| Pairing                 | Orthogonal to paging; can be used inside each chunk/page.                                        | Often paired with a fused kernel (flash-style) while streaming pages.                                                        |\n",
        "\n",
        "## A concrete timeline (what happens in a server)\n",
        "\n",
        "1. **Prefill** (first prompt tokens):\n",
        "\n",
        "   * You usually run **Flash Attention** (or PyTorch SDPA “flash” backend).\n",
        "   * This computes full S×S attention super fast and stores K/V into the **paged KV cache** (pages allocated as needed).\n",
        "\n",
        "2. **Decode** (generate next tokens):\n",
        "\n",
        "   * For each new token, the kernel performs **Paged Attention**:\n",
        "\n",
        "     * walks the sequence’s **page table**,\n",
        "     * reads each K/V **page** in order,\n",
        "     * does attention **page-by-page** with an **online softmax**,\n",
        "     * appends the new token’s K/V to the tail page (allocate a new one if full).\n",
        "   * Inside each page’s compute, you can still use **flash-style fused math**.\n",
        "\n",
        "## Two quick mental models\n",
        "\n",
        "* **Flash = race car engine** (how fast you compute attention on the data in front of you).\n",
        "* **Paged = highway & exits** (how you lay out and fetch the data for many trips without traffic jams/copies).\n",
        "\n",
        "## Minimal code pointers\n",
        "\n",
        "* **Flash (compute):**\n",
        "\n",
        "  ```python\n",
        "  with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_mem_efficient=False, enable_math=False):\n",
        "      out = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "  ```\n",
        "* **Paged decode (compute streaming idea, assuming contiguous K/V):**\n",
        "  (your `paged_decode(q1, k, v, page=256)`—process K/V in chunks and merge with online softmax)\n",
        "* **Paged KV cache (memory layout):**\n",
        "  Real systems (e.g., vLLM) keep K/V in global **pages** with a per-sequence **page table** and an **allocator**. The fused kernel reads directly from those pages (no Python concat) *while* doing the online softmax.\n",
        "\n",
        "## Summary\n",
        "\n",
        "  * **Flash Attention**: *how to compute it fast* (kernel fusion/tiling).\n",
        "  * **Paged Attention**: *how to store & stream K/V at decode* (pages + page tables + online softmax).\n",
        "* They’re **complementary**: Flash for speed, Paged for scalable serving—used together in modern LLM inference.\n"
      ],
      "metadata": {
        "id": "MAGA4yVPEfto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Page Size\n",
        "\n",
        "page=256 is the page size—how many past tokens’ K/V you process at a time in the loop.\n",
        "\n",
        "So with page=256 and context length S:\n",
        "- The loop runs ceil(S / 256) times.\n",
        "- Each iteration computes logits for 256 keys (except the last, which may be smaller), then merges its softmax contribution\n",
        "into the running total using the log-sum-exp trick.\n",
        "   \n",
        "\n",
        "**What does the page size change?**\n",
        "\n",
        "1) Peak memory (temporary activations)\n",
        "\n",
        "Bigger page ⇒ larger temporary tensors (e.g., logits of shape (B, H, 1, page)), so higher peak memory.\n",
        "\n",
        "Smaller page ⇒ smaller temporaries ⇒ lower peak memory.\n",
        "\n",
        "2) Speed / kernel launch overhead\n",
        "\n",
        "Bigger page ⇒ fewer loop iterations ⇒ fewer kernel launches ⇒ usually faster.\n",
        "\n",
        "Smaller page ⇒ more iterations ⇒ more launches / Python overhead ⇒ usually slower.\n",
        "\n",
        "3) Correctness\n",
        "\n",
        "Results are (up to floating-point roundoff) the same regardless of page. The online softmax merge makes paging mathematically equivalent to processing all keys at once.\n",
        "\n",
        "\n",
        "**Rules of thumb**\n",
        "\n",
        "Start with page=256. It’s a good balance on most GPUs for moderate head dims (e.g., D≈64/80/128).\n",
        "\n",
        "If you hit OOM or want to push to longer contexts, drop to 128 (or even 64).\n",
        "\n",
        "If you have headroom and want a bit more speed, try 512 (or 1024 on bigger GPUs).\n",
        "\n",
        "Make page a multiple of 64 or 128 (often maps better to GPU tile sizes).\n",
        "\n",
        "\n",
        "\n",
        "**Edge cases to remember**\n",
        "\n",
        "If S ≤ page, you effectively do one chunk—fastest and highest per-call memory.\n",
        "\n",
        "The last chunk can be shorter than page; the code already handles that.\n",
        "\n",
        "This function is for decode (1 new token). For prefill (full S×S), use SDPA “flash” instead; paging K/V matters much less there.\n",
        "\n",
        "Bottom line: page=256 is just the chunk size for K/V. Increase it for speed (if you have memory), decrease it for memory savings (at some speed cost)."
      ],
      "metadata": {
        "id": "auHHyoBK-YEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise goal: **compute attention for one new token** (the query `q1`) over a **long past context** (`k`, `v`) **in pages** so we keep memory small, while getting **the exact same result** as full softmax.\n",
        "The function below streams through keys/values in **chunks (“pages”)** and uses an **online softmax** to merge each chunk’s contribution **stably**—so you never build the full logits vector.\n",
        "\n",
        "#### Inputs / Output (shapes)\n",
        "\n",
        "* `q1`: **(B, H, 1, D)** — one new query token per batch/head\n",
        "* `k`, `v`: **(B, H, S, D)** — cached keys/values for **S** past tokens\n",
        "* `page`: e.g. **256** — how many past tokens to process per loop\n",
        "* Returns: **(B, H, 1, D)** — the attended value for that new token\n",
        "\n",
        "> B = batch, H = heads, S = context length, D = head size\n",
        "\n",
        "\n",
        "```python\n",
        "B, H, _, D = q1.shape\n",
        "S = k.shape[2]\n",
        "scale = 1.0 / math.sqrt(D)\n",
        "```\n",
        "\n",
        "* Standard attention scale $1/\\sqrt{D}$.\n",
        "\n",
        "We keep **running softmax state** across pages:\n",
        "\n",
        "```python\n",
        "max_log = None  # running max logit  (B,H,1,1)\n",
        "num = None      # running numerator  (B,H,1,D)\n",
        "den = None      # running denominator(B,H,1,1)\n",
        "```\n",
        "\n",
        "Why? If you had all logits $z$ at once, softmax uses:\n",
        "\n",
        "$$\n",
        "\\frac{\\sum_i e^{z_i} V_i}{\\sum_i e^{z_i}}\n",
        "\\quad\\text{(implemented stably as }e^{z_i - m}\\text{ with }m=\\max z\\text{)}\n",
        "$$\n",
        "\n",
        "Paging means we don’t see all $z$ at once, so we **accumulate** numerator/denominator with a **running max** for numerical stability.\n",
        "\n",
        "When we stream pages, we don’t know the global max yet. So we keep a running max m and rebase old sums whenever we see a page with a higher max.\n",
        "\n",
        "What exactly is stored in m?\n",
        "\n",
        "Shape: (B, H, 1, 1) — one value per batch and head (for the single new query).\n",
        "\n",
        "Semantics: the maximum logit seen so far across all pages processed.\n",
        "\n",
        "Now we loop over pages of size `page`:\n",
        "\n",
        "```python\n",
        "for s in range(0, S, page):\n",
        "    kk = k[:, :, s:s+page, :]  # (B,H,chunk,D)\n",
        "    vv = v[:, :, s:s+page, :]  # (B,H,chunk,D)\n",
        "```\n",
        "\n",
        "Compute this page’s logits and its local max:\n",
        "\n",
        "```python\n",
        "logits = torch.matmul(q1, kk.transpose(-2, -1)) * scale  # (B,H,1,chunk)\n",
        "cur_max = logits.amax(dim=-1, keepdim=True)              # (B,H,1,1)\n",
        "```\n",
        "\n",
        "#### First page: initialize the running state\n",
        "\n",
        "```python\n",
        "if max_log is None:\n",
        "    max_log = cur_max\n",
        "    e = torch.exp(logits - max_log)         # (B,H,1,chunk)\n",
        "    num = e @ vv                            # (B,H,1,D)\n",
        "    den = e.sum(dim=-1, keepdim=True)       # (B,H,1,1)\n",
        "```\n",
        "\n",
        "This is just “softmax over what we’ve seen so far” (the first chunk).\n",
        "\n",
        "#### Next pages: merge with **online softmax**\n",
        "\n",
        "```python\n",
        "else:\n",
        "    new_max = torch.maximum(max_log, cur_max)\n",
        "    # rebase old accumulators from max_log ->\n",
        "    \"\"\"\n",
        "    This rebasing is the key: if the new page has a larger max, we scale down the old accumulators so everything is measured relative to the new (bigger) max, keeping exponentials numerically safe.\n",
        "    \"\"\"\n",
        "    num = num * torch.exp(max_log - new_max) + (torch.exp(logits - new_max) @ vv)\n",
        "    den = den * torch.exp(max_log - new_max) + torch.exp(logits - new_max).sum(dim=-1, keepdim=True)\n",
        "    max_log = new_max\n",
        "```\n",
        "\n",
        "**What’s happening:**\n",
        "Suppose old accumulators were based on $m_{\\text{old}}$ and the new page has max $m_{\\text{page}}$. For stability, switch to $m_{\\text{new}}=\\max(m_{\\text{old}}, m_{\\text{page}})$. Then\n",
        "\n",
        "$$\n",
        "\\sum e^{z - m_{\\text{new}}}\n",
        "= e^{m_{\\text{old}} - m_{\\text{new}}}\\!\\!\\sum_{\\text{old}}\\! e^{z - m_{\\text{old}}}\n",
        "+ \\sum_{\\text{page}} e^{z - m_{\\text{new}}}\n",
        "$$\n",
        "\n",
        "Same scaling for the numerator $\\sum e^{z - m} V$. That’s exactly what those lines implement.\n",
        "\n",
        "After all pages:\n",
        "\n",
        "```python\n",
        "return num / den   # (B,H,1,D)\n",
        "```\n",
        "\n",
        "That’s the **softmax-weighted sum of V over the entire context**, identical to doing it in one shot—just streamed.\n",
        "\n",
        "\n",
        "#### Tiny numeric example\n",
        "\n",
        "Suppose after page 1:\n",
        "\n",
        "* `max_log = 5`\n",
        "* `den = ∑ e^{z_old - 5}`\n",
        "* `num = ∑ e^{z_old - 5} V_old`\n",
        "\n",
        "Page 2 has `cur_max = 7` → `new_max = 7`. To merge:\n",
        "\n",
        "* Rescale old terms by `e^{5-7}`:\n",
        "\n",
        "  * `den ← den * e^{5-7} + ∑ e^{z_new - 7}`\n",
        "  * `num ← num * e^{5-7} + ∑ e^{z_new - 7} V_new`\n",
        "* Set `max_log = 7`.\n",
        "\n",
        "At the end, the output is `num / den`, which equals the full softmax result.\n",
        "\n",
        "`m` is the **running per-(B,H) max logit** used to do a **stable, streaming softmax** across pages. It lets you combine chunks without ever building all logits at once, while avoiding numerical issues.\n",
        "\n",
        "#### Intuition (no equations)\n",
        "\n",
        "* Think of having **100,000** past tokens. Instead of reading all keys at once, you read **a few hundred at a time**.\n",
        "* For each chunk, you compute “how much this chunk contributes” and **blend** it with what you already had.\n",
        "* The **running max** lets you keep numbers well-scaled so the softmax doesn’t blow up or underflow.\n",
        "\n",
        "\n",
        "#### Why no causal mask here?\n",
        "\n",
        "This is **decode**: K/V contain **only past tokens** for the current position. There are no “future” tokens to mask, so `is_causal` isn’t needed.\n",
        "\n",
        "\n",
        "#### What does `page=256` change?\n",
        "\n",
        "* **Bigger** page (e.g., 512/1024) → fewer iterations, usually **faster**, but **higher** temporary memory.\n",
        "* **Smaller** page (e.g., 64/128) → lower peak memory, but more loops/overhead → **slower**.\n",
        "\n",
        "\n",
        "#### Micro example (one head, tiny numbers)\n",
        "\n",
        "Say logits for all S keys were `[2, 1, 3, 0]`, and `page=2`.\n",
        "\n",
        "* Page 1 logits: `[2, 1]` → init `num, den` with those.\n",
        "* Page 2 logits: `[3, 0]` → compute its exponentials **relative to a new overall max (3)**, **rescale** old `num/den` to the new max, then add this page’s contribution.\n",
        "* Final `num/den` equals softmax over `[2,1,3,0]` in one shot.\n",
        "\n",
        "\n",
        "#### Summary\n",
        "\n",
        "* **What:** Compute attention for a single new token by **streaming** K/V in pages and **merging softmax** chunk by chunk.\n",
        "* **Why:** Keep **peak memory low**, perfect for **long contexts** at decode.\n",
        "* **Correctness:** Same result as full softmax, thanks to the **online (log-sum-exp) merge**.\n"
      ],
      "metadata": {
        "id": "5j7QvdUmJjDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 2) Paged attention (decode): 1 new token attends over K/V in pages ----------\n",
        "@torch.no_grad()\n",
        "def paged_decode(q1, k, v, page=256):\n",
        "    # Online softmax over pages: O(S) memory, low peak activation.\n",
        "    B, H, _, D = q1.shape  # running max of logits (per B,H,1) to keep exps stable\n",
        "    S = k.shape[2] # running numerator: sum_j exp(logit_j - max) * V_j\n",
        "    scale = 1.0 / math.sqrt(D)\n",
        "\n",
        "    max_log = None\n",
        "    num = None\n",
        "    den = None\n",
        "\n",
        "    for s in range(0, S, page):\n",
        "        kk = k[:, :, s:s+page, :]\n",
        "        vv = v[:, :, s:s+page, :]\n",
        "        logits = torch.matmul(q1, kk.transpose(-2, -1)) * scale  # (B,H,1,chunk)\n",
        "        cur_max = logits.amax(dim=-1, keepdim=True)              # (B,H,1,1)\n",
        "\n",
        "        if max_log is None:\n",
        "            max_log = cur_max\n",
        "            e = torch.exp(logits - max_log)\n",
        "            num = e @ vv\n",
        "            den = e.sum(dim=-1, keepdim=True)\n",
        "        else:\n",
        "            new_max = torch.maximum(max_log, cur_max)\n",
        "            # merge old accumulators with new chunk (log-sum-exp trick)\n",
        "            num = num * torch.exp(max_log - new_max) + (torch.exp(logits - new_max) @ vv)\n",
        "            den = den * torch.exp(max_log - new_max) + torch.exp(logits - new_max).sum(dim=-1, keepdim=True)\n",
        "            max_log = new_max\n",
        "    return num / den"
      ],
      "metadata": {
        "id": "ruTRXsyKhJqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What I showed earlier (`paged_decode(q1, k, v, page=256)`) **doesn’t implement that memory system**. In my demo:\n",
        "\n",
        "* `page=256` is just a **compute chunk size**: we iterate through a *contiguous* `k, v` tensor in slices to keep activations small.\n",
        "* Real “paged attention” adds a **KV cache layout + allocator** on top of the math so you **don’t even need K/V to be contiguous per sequence**.\n",
        "\n",
        "Here’s the difference, side by side:\n",
        "\n",
        "#### 1) What our demo does (compute paging)\n",
        "\n",
        "* Input: contiguous `k, v` of shape `(B, H, S, D)`.\n",
        "* We loop over `s : s+page` to **compute** softmax in chunks and merge with online softmax.\n",
        "* Pros: simple, teaches the math.\n",
        "* Limitation: assumes you already **have** K/V for the sequence in one contiguous block.\n",
        "\n",
        "#### 2) What real paged KV cache does (memory paging)\n",
        "\n",
        "* Global K/V storage is a big pool split into **fixed-size pages**, e.g.:\n",
        "\n",
        "  ```\n",
        "  kv_cache_k: (NUM_PAGES, H, PAGE_SIZE, D)\n",
        "  kv_cache_v: (NUM_PAGES, H, PAGE_SIZE, D)\n",
        "  ```\n",
        "* Each sequence keeps a **page table** listing which pages hold its tokens, plus how many slots are filled in the last page:\n",
        "\n",
        "  ```\n",
        "  seq.page_ids = [p0, p1, p2, ...]     # which pages belong to this seq\n",
        "  seq.tail_filled = r                   # 0..PAGE_SIZE-1\n",
        "  ```\n",
        "* **Append**: write new tokens into the current tail page; when full, **allocate** a fresh page from a free list and append its id to `page_ids`.\n",
        "* **Finish**: when the sequence ends, **return** its pages to the free list (reuse for other requests).\n",
        "* **Continuous batching**: Because every sequence has its own page table, you can batch requests with **different lengths** without padding or copying large K/V blocks.\n",
        "* **Attention read**: kernels read K/V **directly from the listed pages** (no concat/copy), computing attention over a *logical* contiguous stream formed by those pages.\n",
        "\n",
        "\n",
        "> In **real** paged attention (vLLM/FlashInfer), step **(1)** is *not done with Python concat*. The fused CUDA kernel:\n",
        ">\n",
        "> * walks the **page table**,\n",
        "> * reads from each page in order,\n",
        "> * does the softmax-and-weighted-sum **on the fly**,\n",
        ">   so you avoid copies **and** keep memory + latency low.\n",
        "\n",
        "#### Where the **dynamic allocation/reuse** lives\n",
        "\n",
        "* A tiny **allocator** manages a `free_pages` list.\n",
        "* On append:\n",
        "\n",
        "  * If the current tail page has space, write into it.\n",
        "  * Else pop one page from `free_pages`, push its id into the sequence’s `page_ids`.\n",
        "* On finish/cancel: push all `page_ids` back into `free_pages`.\n",
        "* This is how servers **continuously batch** requests that grow to different lengths without expensive K/V copies.\n",
        "\n",
        "\n",
        "#### Summary\n",
        "\n",
        "* My earlier `paged_decode(q1, k, v, page=256)` showed **compute paging** (small-chunk softmax merge) assuming contiguous K/V.\n",
        "* Real **paged KV cache** also includes a **memory layout + allocator + page tables** so sequences of different lengths can **share, extend, and free** K/V **without copying**.\n",
        "* Production systems use **fused kernels** that read directly from those pages while doing the **same online-softmax math** you saw in the demo.\n"
      ],
      "metadata": {
        "id": "ce3eRK-XFm2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 3) SDPA wrapper (PyTorch built-in) ----------\n",
        "def sdpa(q, k, v, *, causal=True, attn_mask=None):\n",
        "    return F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask, is_causal=causal)\n"
      ],
      "metadata": {
        "id": "qUlzQvYNhN3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 4) Quick benchmark ----------\n",
        "def time_cuda(fn, warmup=5, iters=20):\n",
        "    # warmup\n",
        "    for _ in range(warmup): fn()\n",
        "    torch.cuda.synchronize()\n",
        "    start = torch.cuda.Event(True); end = torch.cuda.Event(True)\n",
        "    start.record()\n",
        "    for _ in range(iters): fn()\n",
        "    end.record(); torch.cuda.synchronize()\n",
        "    return start.elapsed_time(end) / iters  # ms/call\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\"\"\"\n",
        "Convenience hyperparameters you’ll use to shape tensors:\n",
        "\n",
        "B = batch size\n",
        "\n",
        "H = number of attention heads\n",
        "\n",
        "S = number of tokens in the sequence (how many time steps you’re attending over)\n",
        "\n",
        "D = per-head hidden size (head dimension)\n",
        "\n",
        "\"\"\"\n",
        "B, H, D = 1, 8, 64"
      ],
      "metadata": {
        "id": "C1gNNlOChSzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Prefill (full SxS): compare SDPA vs simple flash-like --\n",
        "SEQ_LIST = [128, 256, 512, 1024]\n",
        "print(\"=== Prefill (full attention) — SDPA vs flash_attn_simple ===\")\n",
        "for S in SEQ_LIST:\n",
        "    q = torch.randn(B,H,S,D, device=device, dtype=dtype)\n",
        "    k = torch.randn(B,H,S,D, device=device, dtype=dtype)\n",
        "    v = torch.randn(B,H,S,D, device=device, dtype=dtype)\n",
        "\n",
        "    t_sdpa = time_cuda(lambda: sdpa(q,k,v, causal=True), warmup=3, iters=10)\n",
        "    t_flash = time_cuda(lambda: flash_attn_simple(q,k,v, causal=True, q_block=128, k_block=128),\n",
        "                        warmup=2, iters=5)  # a bit heavier in Python, fewer iters\n",
        "\n",
        "    tokps_sdpa  = (B*S) / (t_sdpa/1e3)   # tokens/sec consumed\n",
        "    tokps_flash = (B*S) / (t_flash/1e3)\n",
        "    print(f\"S={S:4d} | SDPA: {t_sdpa:6.2f} ms  ({tokps_sdpa:8.0f} tok/s)  |  flash-simple: {t_flash:6.2f} ms  ({tokps_flash:8.0f} tok/s)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSdnotnZhW9s",
        "outputId": "d2bf5862-9216-4675-e8f3-9ceae35f9924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Prefill (full attention) — SDPA vs flash_attn_simple ===\n",
            "S= 128 | SDPA:   0.30 ms  (  422913 tok/s)  |  flash-simple:   0.60 ms  (  212832 tok/s)\n",
            "S= 256 | SDPA:   0.31 ms  (  821946 tok/s)  |  flash-simple:   2.03 ms  (  125931 tok/s)\n",
            "S= 512 | SDPA:   0.88 ms  (  579729 tok/s)  |  flash-simple:   7.72 ms  (   66363 tok/s)\n",
            "S=1024 | SDPA:   2.70 ms  (  379701 tok/s)  |  flash-simple:  29.67 ms  (   34514 tok/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Decode (1 new token over context S): compare SDPA vs paged_decode --\n",
        "CTX_LIST = [512, 1024, 2048, 4096]\n",
        "print(\"\\n=== Decode (1 token) — SDPA vs paged_decode (page=256) ===\")\n",
        "for S in CTX_LIST:\n",
        "    q1 = torch.randn(B,H,1,D, device=device, dtype=dtype)\n",
        "    k = torch.randn(B,H,S,D, device=device, dtype=dtype)\n",
        "    v = torch.randn(B,H,S,D, device=device, dtype=dtype)\n",
        "\n",
        "    t_sdpa = time_cuda(lambda: sdpa(q1,k,v, causal=False), warmup=5, iters=50 if S<=2048 else 25)\n",
        "    t_paged = time_cuda(lambda: paged_decode(q1,k,v, page=256), warmup=5, iters=50 if S<=2048 else 25)\n",
        "\n",
        "    tokps_sdpa  = 1 / (t_sdpa/1e3)   # tokens/sec generated (1 token per call)\n",
        "    tokps_paged = 1 / (t_paged/1e3)\n",
        "    print(f\"S={S:4d} | SDPA: {t_sdpa:7.3f} ms  ({tokps_sdpa:8.1f} tok/s)  |  paged: {t_paged:7.3f} ms  ({tokps_paged:8.1f} tok/s)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Px-QYnEhbFu",
        "outputId": "f26ba4e9-3ded-4257-81ec-63974913209e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Decode (1 token) — SDPA vs paged_decode (page=256) ===\n",
            "S= 512 | SDPA:   0.188 ms  (  5306.2 tok/s)  |  paged:   0.460 ms  (  2174.6 tok/s)\n",
            "S=1024 | SDPA:   0.184 ms  (  5421.3 tok/s)  |  paged:   1.061 ms  (   942.8 tok/s)\n",
            "S=2048 | SDPA:   0.259 ms  (  3855.7 tok/s)  |  paged:   2.513 ms  (   397.9 tok/s)\n",
            "S=4096 | SDPA:   0.405 ms  (  2470.2 tok/s)  |  paged:   4.528 ms  (   220.9 tok/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For folks who don't have GPU, please use CPU version of the code below"
      ],
      "metadata": {
        "id": "jZo3EpVISoGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== CPU prefill benchmark: SDPA vs flash_attn_simple ====\n",
        "# CPU setup\n",
        "device = \"cpu\"\n",
        "dtype = torch.float32\n",
        "torch.manual_seed(0)\n",
        "# Optional: stabilize timings by using 1 thread (or set a fixed number you prefer)\n",
        "# torch.set_num_threads(1)\n",
        "\n",
        "def time_cpu(fn, warmup=3, iters=10):\n",
        "    # Warmup\n",
        "    for _ in range(warmup):\n",
        "        fn()\n",
        "    # Measure\n",
        "    t0 = time.perf_counter()\n",
        "    for _ in range(iters):\n",
        "        fn()\n",
        "    t1 = time.perf_counter()\n",
        "    return (t1 - t0) * 1000.0 / iters  # ms per call\n",
        "\n",
        "# Minimal SDPA wrapper (CPU path uses PyTorch's math kernel)\n",
        "def sdpa(q, k, v, *, causal=True):\n",
        "    return F.scaled_dot_product_attention(q, k, v, is_causal=causal)\n",
        "\n",
        "# ---- Benchmark params (keep modest on CPU) ----\n",
        "B, H, D = 1, 8, 64\n",
        "SEQ_LIST = [64, 128, 256, 512]  # reduce or increase as your CPU allows\n",
        "\n",
        "print(\"=== Prefill (full attention, CPU) — SDPA vs flash_attn_simple ===\")\n",
        "for S in SEQ_LIST:\n",
        "    q = torch.randn(B, H, S, D, device=device, dtype=dtype)\n",
        "    k = torch.randn(B, H, S, D, device=device, dtype=dtype)\n",
        "    v = torch.randn(B, H, S, D, device=device, dtype=dtype)\n",
        "\n",
        "    # SDPA timing\n",
        "    t_sdpa = time_cpu(lambda: sdpa(q, k, v, causal=True), warmup=2, iters=5)\n",
        "\n",
        "    # flash_attn_simple timing (Python loops; slower on CPU)\n",
        "    # Use smaller blocks on CPU to keep memory in check; speed will still be lower than SDPA\n",
        "    t_flash = time_cpu(\n",
        "        lambda: flash_attn_simple(q, k, v, causal=True, q_block=64, k_block=64),\n",
        "        warmup=1, iters=2\n",
        "    )\n",
        "\n",
        "    tokps_sdpa  = (B * S) / (t_sdpa / 1e3)   # tokens/sec \"consumed\"\n",
        "    tokps_flash = (B * S) / (t_flash / 1e3)\n",
        "\n",
        "    print(f\"S={S:4d} | SDPA: {t_sdpa:7.2f} ms  ({tokps_sdpa:8.0f} tok/s)  |  flash-simple: {t_flash:7.2f} ms  ({tokps_flash:8.0f} tok/s)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzB1QEP3SW2E",
        "outputId": "43e5543c-3951-46a3-8bbf-ee67167fbb98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Prefill (full attention, CPU) — SDPA vs flash_attn_simple ===\n",
            "S=  64 | SDPA:    0.21 ms  (  301979 tok/s)  |  flash-simple:    1.35 ms  (   47395 tok/s)\n",
            "S= 128 | SDPA:    0.75 ms  (  170197 tok/s)  |  flash-simple:    3.21 ms  (   39891 tok/s)\n",
            "S= 256 | SDPA:    3.05 ms  (   83884 tok/s)  |  flash-simple:   13.77 ms  (   18592 tok/s)\n",
            "S= 512 | SDPA:   12.63 ms  (   40527 tok/s)  |  flash-simple:   52.15 ms  (    9817 tok/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Benchmark params (keep modest on CPU) ----\n",
        "B, H, D = 1, 8, 64\n",
        "CTX_LIST = [256, 512, 1024]   # increase if your CPU is fast\n",
        "\n",
        "print(\"\\n=== Decode (CPU): SDPA vs paged_decode (page=128) ===\")\n",
        "for S in CTX_LIST:\n",
        "    q1 = torch.randn(B, H, 1, D, device=device, dtype=dtype)      # one new token\n",
        "    k  = torch.randn(B, H, S, D, device=device, dtype=dtype)      # past keys\n",
        "    v  = torch.randn(B, H, S, D, device=device, dtype=dtype)      # past values\n",
        "\n",
        "    t_sdpa  = time_cpu(lambda: sdpa(q1, k, v, causal=False), warmup=3, iters=20)\n",
        "    t_paged = time_cpu(lambda: paged_decode(q1, k, v, page=128), warmup=3, iters=20)\n",
        "\n",
        "    tokps_sdpa  = 1 / (t_sdpa / 1e3)   # tokens/sec (1 token per call)\n",
        "    tokps_paged = 1 / (t_paged / 1e3)\n",
        "\n",
        "    print(f\"S={S:4d} | SDPA: {t_sdpa:7.2f} ms  ({tokps_sdpa:7.1f} tok/s)  |  paged: {t_paged:7.2f} ms  ({tokps_paged:7.1f} tok/s)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qM5-aGzYTOOq",
        "outputId": "57afa97c-316f-48cc-9cdf-98dcba6001d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Decode (CPU): SDPA vs paged_decode (page=128) ===\n",
            "S= 256 | SDPA:    0.06 ms  (16257.3 tok/s)  |  paged:    0.34 ms  ( 2919.5 tok/s)\n",
            "S= 512 | SDPA:    0.12 ms  ( 8472.3 tok/s)  |  paged:    0.66 ms  ( 1509.1 tok/s)\n",
            "S=1024 | SDPA:    0.23 ms  ( 4316.8 tok/s)  |  paged:    1.34 ms  (  747.9 tok/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise (after running functions above)\n",
        "- Try different page sizes (128/256/512). Log the observations. Which is faster on your GPU? What trade-off does page control?\n",
        "-  In prefill, how does ms scale as S doubles (≈4× slower or ≈2× slower)? Why\n",
        "- In decode, does ms/token grow ~linearly with S for both SDPA and Paged?\n",
        "-  Tweak these:\n",
        "B_list = [1, 2],\n",
        "H_list = [8, 16],\n",
        "D_list = [64, 128]\n",
        "and report observations\n",
        "- For folks who can use both GPU and CPU version, what observations do you see?"
      ],
      "metadata": {
        "id": "gS2aCK9HO9Ku"
      }
    }
  ]
}